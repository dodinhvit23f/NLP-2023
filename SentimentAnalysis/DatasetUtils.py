import csv
import re
import py_vncorenlp
import torch

from transformers import AutoTokenizer, AutoModel


VN_CHARS_LOWER = u'แบกแบฃรฃรรกรขแบญแบงแบฅแบฉแบซฤแบฏแบฑแบทแบณแบตรณรฒแปรตแปรดแปแปแปแปแปฦกแปแปแปฃแปแปกรฉรจแบปแบนแบฝรชแบฟแปแปแปแปรบรนแปฅแปงลฉฦฐแปฑแปฏแปญแปซแปฉรญรฌแปแปฤฉรฝแปณแปทแปตแปนฤรฐ'
VN_CHARS_UPPER = u'แบแบขรรรรแบฌแบฆแบคแบจแบชฤแบฎแบฐแบถแบฒแบดรรแปรแปรแปแปแปแปแปฦแปแปแปขแปแปรรแบบแบธแบผรแบพแปแปแปแปรรแปคแปฆลจฦฏแปฐแปฎแปฌแปชแปจรรแปแปฤจรแปฒแปถแปดแปธรฤ'
VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER

replace_list = {
        'รฒa': 'oร', 'รณa': 'oรก', 'แปa': 'oแบฃ', 'รตa': 'oรฃ', 'แปa': 'oแบก', 'รฒe': 'oรจ', 'รณe': 'oรฉ', 'แปe': 'oแบป',
        'รตe': 'oแบฝ', 'แปe': 'oแบน', 'รนy': 'uแปณ', 'รบy': 'uรฝ', 'แปงy': 'uแปท', 'ลฉy': 'uแปน', 'แปฅy': 'uแปต', 'uแบฃ': 'แปงa',
        'aฬ': 'แบฃ', 'รดฬ': 'แป', 'uยด': 'แป', 'รดฬ': 'แป', 'รดฬ': 'แป', 'รดฬ': 'แป', 'รขฬ': 'แบฅ', 'รขฬ': 'แบซ', 'รขฬ': 'แบฉ',
        'รขฬ': 'แบง', 'oฬ': 'แป', 'รชฬ': 'แป', 'รชฬ': 'แป', 'ฤฬ': 'แบฏ', 'uฬ': 'แปง', 'รชฬ': 'แบฟ', 'ฦกฬ': 'แป', 'iฬ': 'แป',
        'eฬ': 'แบป', 'รk': u' ร ', 'aห': 'ร', 'iห': 'รฌ', 'ฤยด': 'แบฏ', 'ฦฐฬ': 'แปญ', 'eห': 'แบฝ', 'yห': 'แปน', 'aยด': 'รก',
        ':))': '', ':)': '', 'รด kรชi': ' ok ', 'okie': ' ok ', ' o kรช ': ' ok ',
        'okey': ' ok ', 'รดkรช': ' ok ', 'oki': ' ok ', ' oke ': ' ok ', ' okay': ' ok ', 'okรช': ' ok ',
        ' tks ': u' cรกm ฦกn ', 'thks': u' cรกm ฦกn ', 'thanks': u' cรกm ฦกn ', 'ths': u' cรกm ฦกn ', 'thank': u' cรกm ฦกn ',
        'โญ': 'star ', '*': 'star ', '๐': 'star ',
        'kg ': u' khรดng ', 'not': u' khรดng ', u' kg ': u' khรดng ', '"k ': u' khรดng ', ' kh ': u' khรดng ',
        'kรด': u' khรดng ', 'hok': u' khรดng ', ' kp ': u' khรดng phแบฃi ', u' kรด ': u' khรดng ', '"ko ': u' khรดng ',
        u' ko ': u' khรดng ', u' k ': u' khรดng ', 'khong': u' khรดng ', u' hok ': u' khรดng ',
        'he he': ' tแปt ', 'hehe': ' tแปt ', 'hihi': ' tแปt ', 'haha': ' tแปt ', 'hjhj': ' tแปt ',
        ' lol ': ' tแป ', ' cc ': ' tแป ', 'cute': u' dแป thฦฐฦกng ', 'huhu': ' tแป ', ' vs ': u' vแปi ',
        'wa': ' quรก ', 'wรก': u' quรก', 'j': u' gรฌ ', 'โ': ' ',
        ' sz ': u' cแปก ', 'size': u' cแปก ', u' ฤx ': u' ฤฦฐแปฃc ', 'dk': u' ฤฦฐแปฃc ', 'dc': u' ฤฦฐแปฃc ', 'ฤk': u' ฤฦฐแปฃc ',
        'ฤc': u' ฤฦฐแปฃc ', 'authentic': u' chuแบฉn chรญnh hรฃng ', u' aut ': u' chuแบฉn chรญnh hรฃng ',
        u' auth ': u' chuแบฉn chรญnh hรฃng ', 'thick': u' tแปt ', 'store': u' cแปญa hรng ',
        'shop': u' cแปญa hรng ', 'sp': u' sแบฃn phแบฉm ', 'gud': u' tแปt ', 'god': u' tแปt ', 'wel done': ' tแปt ',
        'good': u' tแปt ', 'gรบt': u' tแปt ',
        'sแบฅu': u' xแบฅu ', 'gut': u' tแปt ', u' tot ': u' tแปt ', u' nice ': u' tแปt ', 'perfect': 'rแบฅt tแปt',
        'bt': u' bรฌnh thฦฐแปng ',
        'time': u' thแปi gian ', 'qรก': u' quรก ', u' ship ': u' giao hรng ', u' m ': u' mรฌnh ', u' mik ': u' mรฌnh ',
        'รชฬ': 'แป', 'product': 'sแบฃn phแบฉm', 'quality': 'chแบฅt lฦฐแปฃng', 'chat': ' chแบฅt ', 'excelent': 'hoรn hแบฃo',
        'bad': 'tแป', 'fresh': ' tฦฐฦกi ', 'sad': ' tแป ',
        'date': u' hแบกn sแปญ dแปฅng ', 'hsd': u' hแบกn sแปญ dแปฅng ', 'quickly': u' nhanh ', 'quick': u' nhanh ',
        'fast': u' nhanh ', 'delivery': u' giao hรng ', u' sรญp ': u' giao hรng ',
        'beautiful': u' ฤแบนp tuyแปt vแปi ', u' tl ': u' trแบฃ lแปi ', u' r ': u' rแปi ', u' shopE ': u' cแปญa hรng ',
        u' order ': u' ฤแบทt hรng ',
        'chแบฅt lg': u' chแบฅt lฦฐแปฃng ', u' sd ': u' sแปญ dแปฅng ', u' dt ': u' ฤiแปn thoแบกi ', u' nt ': u' nhแบฏn tin ',
        u' tl ': u' trแบฃ lแปi ', u' sรi ': u' xรi ', u'bjo': u' bao giแป ',
        'thik': u' thรญch ', u' sop ': u' cแปญa hรng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rแบฅt ',
        u'quแบฃ ng ': u' quแบฃng  ',
        'dep': u' ฤแบนp ', u' xau ': u' xแบฅu ', 'delicious': u' ngon ', u'hรg': u' hรng ', u'qแปงa': u' quแบฃ ',
        'iu': u' yรชu ', 'fake': u' giแบฃ mแบกo ', 'trl': 'trแบฃ lแปi', '><': u' tแปt ',
        ' por ': u' tแป ', ' poor ': u' tแป ', 'ib': u' nhแบฏn tin ', 'rep': u' trแบฃ lแปi ', u'fback': ' feedback ',
        'fedback': ' feedback ',
        # dฦฐแปi 3* quy vแป 1*, trรชn 3* quy vแป 5*
        '6 sao': ' 5 star ', '6 star': ' 5 star', '5star': ' 5star ', '5 sao': ' 5star ', '5sao': ' 5star ',
        'starstarstarstarstar': ' 5 star ', '1 sao': ' 1 star ', '1sao': ' 1 star ', '2 sao': ' 1 star ', '2sao': ' 1 star ',
        '2 starstar': ' 1 star ', '1star': ' 1 star ', '0 sao': ' 1star ', '0star': ' 1 star ', }

def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1E0-\U0001F1FF"
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

def loadDataSet(path):

    tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2", padding=True, max_length=1000 )
    rdrsegmenter = py_vncorenlp.VnCoreNLP(annotators=["wseg"])

    text = []
    labels = []
    max_padding = 128
    #model = AutoModel.from_pretrained("vinai/phobert-base-v2")
    skip = False

    with open(path, newline='') as f:
        reader = csv.reader(f, delimiter='\t', quoting=csv.QUOTE_NONE)
        for row in reader:
            if skip:
                line = deEmojify(row[1])

                for k, v in replace_list.items():
                    line = line.replace(k, v)

                x = torch.tensor([tokenizer.encode(' '.join(rdrsegmenter.word_segment(line)))])
                padding = torch.zeros(1, max_padding - x.shape[1], dtype=torch.int64)
                x = torch.concat((x, padding), dim=1)
                text.append(x)

                y_true = torch.zeros(3, dtype=torch.float)
                y_true[int(row[2])] = 1
                y_true = y_true.reshape(-1, 1).t()
                labels.append(y_true)
            skip = True
    return text, labels



if __name__ == '__main__':

    loadDataSet("./data/train/train_VLSP.csv")
